Carry a spoken conversation with large language models! This project uses Whisper speech to text to transcribe the user's voice, sending the text to an LLM, then chunking the LLM's output into sentences and finally have Piper generate audio. This process happens in <600ms on an RTX 3090.

## Dependencies
- `Docker Compose v2` on the server you want to do all the processing
- `pyenv` or install `Python3.11` manually on the client

## Install
### Client
If you have `pyenv` installed
1. `pyenv install 3.11 --skip-existing`
2. `pyenv global 3.11.14`
3. `pyenv exec python3.11 -m venv venv`

If you already have `python3.11`
1. `python3 -m venv venv`

Now. 
1. `source venv/bin/activate` (Linux) or `.\venv\Scripts\Activate.ps1` (Windows)
2. `pip install -r requirements.txt`
3. `python3 main.py`

### Server
Where the `docker-compose.yml` is run `docker compose up -d`.

## Features
- 100% offline, opensource and private
- Wake word detection: 'Hey Jarvis'
- Hands free interation
- Client server model
- Fully multilingual pipeline
- Streamed reponses

## Configuring
- General configuration changes are made at `src/llm-voice-assistant-client/config.toml`.
- To change Piper models edit `src/llm-voice-assistant-client/tts-models/piper-models.json` set `autostart` and `enabled` to false for the model in your language. Then set these to true for the one you want to use.
- In order to change LLM models you need to add it to `src/llm-voice-assistant-server/docker-compose.yml` in the configs section. This is [llama-swap's](https://github.com/mostlygeek/llama-swap) config. You also need to edit `src/llm-voice-assistant-client/config.toml` to tell it to use the proper model.
- Speech to text is the same way. Edit `src/llm-voice-assistant-server/docker-compose.yml` to use a different Whisper model which by default is `rtlingo/mobiuslabsgmbh-faster-whisper-large-v3-turbo`. Then also change `src/llm-voice-assistant-client/config.toml`.

## Multilinguality
The language has to be supported by STT ([Whisper languages](https://github.com/openai/whisper#available-models-and-languages)), the LLM and TTS ([Piper languages](https://github.com/rhasspy/piper#Voices)).

The output text generated by the LLM is chunked into sentences and ran aganist [Lingua](https://github.com/pemistahl/lingua-py) to detect the language. A TTS model for that language is then downloaded and cached for future use. This unfortunately means that the first time a language is used the time it takes to generate a response back to the user will be exetremely slow. This is done to save on memory as loading all of the TTS models can take ~2.5GB of memory.
